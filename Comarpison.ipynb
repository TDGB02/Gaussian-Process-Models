{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from GP_Classes import GaussianProcess, SparseGaussianProcess\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_memory_usage(n_samples, n_features, model_type, n_inducing=None):\n",
    "    # Memory for dataset (X_train and y_train)\n",
    "    dataset_memory = n_samples * n_features * 8 + n_samples * 8  # X_train + y_train (float64, 8 bytes per float)\n",
    "    if model_type == \"GPR\":\n",
    "        # Full GP: Covariance matrix is N x N\n",
    "        covariance_matrix_memory = n_samples ** 2 * 8  # N x N matrix, 8 bytes per float\n",
    "    elif model_type == \"SGPR\":\n",
    "        # Sparse GP: Covariance matrix is M x M (inducing points)\n",
    "        if n_inducing is None:\n",
    "            raise ValueError(\"n_inducing must be provided for SGPR\")\n",
    "        covariance_matrix_memory = n_inducing ** 2 * 8  # M x M matrix, 8 bytes per float\n",
    "    \n",
    "    # Memory for kernel parameters and gradients (assume around 1 KB for kernel + gradients)\n",
    "    kernel_and_gradients_memory = 1024  # Rough estimation\n",
    "\n",
    "    # Total memory usage estimate\n",
    "    total_memory = dataset_memory + covariance_matrix_memory + kernel_and_gradients_memory\n",
    "\n",
    "    return total_memory / (1024 ** 2)  # Convert to MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for farthest point sampling to select inducing points:\n",
    "def farthest_point_sampling(X, n_inducing_points):\n",
    "    \n",
    "    # Start by selecting a random point from X as the first inducing point\n",
    "    Z = [X[np.random.choice(len(X))]]\n",
    "    \n",
    "    # Iteratively select the farthest point from the set Z and and to the set\n",
    "    for _ in range(n_inducing_points - 1):\n",
    "        # Calculate distances from each point in X to the nearest point in Z\n",
    "        dists = jnp.min(jnp.array([jnp.linalg.norm(X - z, axis=1) for z in Z]), axis=0)\n",
    "        # Select the point farthest from Z and add it to the set\n",
    "        next_point = X[jnp.argmax(dists)]\n",
    "        Z.append(next_point)\n",
    "    \n",
    "    return jnp.array(Z)\n",
    "\n",
    "# Function to train a GP model and return the RMSE, average uncertainty, time taken, and iteration count\n",
    "def train_and_evaluate_gp(n_samples, model, optim_method):\n",
    "    # Generate synthetic regression data\n",
    "    X, y = make_regression(n_samples=n_samples, n_features=1, noise=1, random_state=0)  # n_features=1 for plotting\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Measure the time taken to optimize (which includes matrix inversion)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Optimize the model's parameters with a callback to count iterations\n",
    "    if model == \"GPR\":\n",
    "        model = GaussianProcess(X_train, y_train, optim_method)\n",
    "    if model == \"SGPR\":\n",
    "        n_inducing_points = int(len(X_train)*0.1)  # 10% of training points\n",
    "        # Z_train = X_train[np.random.choice(len(X_train), n_inducing_points, replace=False)]\n",
    "        Z_train = farthest_point_sampling(X_train, n_inducing_points)\n",
    "        model = SparseGaussianProcess(X_train, y_train,Z_train, optim_method)\n",
    "    \n",
    "    noise = model.fit()\n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Time taken for matrix inversion and optimization\n",
    "    time_taken = end_time - start_time\n",
    "    \n",
    "    # Retrieve and display parameter history if optimizer is ADAM\n",
    "    if hasattr(model, \"adam_optimizer\"):\n",
    "        param_history = model.adam_optimizer.param_history\n",
    "        print(\"Parameter History:\")\n",
    "        #for params in param_history:\n",
    "\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    means, variances = zip(*(model.predict(x.reshape(-1, 1)) for x in X_test))\n",
    "\n",
    "    # Compute RMSE\n",
    "    rmse = np.sqrt(np.mean((np.array(means) - y_test) ** 2))\n",
    "    avg_uncertantiy = np.mean(np.sqrt(np.array(variances)))\n",
    "    # Calculate the lower and upper bounds of the confidence interval\n",
    "    lower_bound = means - 1.96 * np.sqrt(variances)\n",
    "    upper_bound = means + 1.96 * np.sqrt(variances)\n",
    "    # Check if y_test is within the confidence interval bounds\n",
    "    uncertainties = np.where((lower_bound <= y_test) & (y_test <= upper_bound), 1, 0)\n",
    "    in_interval = np.mean(uncertainties)\n",
    "\n",
    "    return rmse, avg_uncertantiy, time_taken, noise, in_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import psutil\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Utility to get memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 ** 2)  # Return memory usage in MB\n",
    "\n",
    "# General function to run comparison for multiple GP models\n",
    "def run_comparison(max_samples, models_kernels):\n",
    "    df = pd.DataFrame(columns=['Model', 'RMSE', 'Dataset Size', 'Time', 'Memory Usage', 'Avg Uncertainty', 'Predictive Noise', 'In Interval'])\n",
    "    start = 50\n",
    "    step = 50\n",
    "    dataset_sizes = np.arange(start, max_samples + 1, step)\n",
    "\n",
    "    # Dictionary to store results for each model\n",
    "    results = {\n",
    "        'loss_history': {},\n",
    "        'times': {},\n",
    "        'uncertainties': {},\n",
    "        'memory_usages': {},\n",
    "    }\n",
    "\n",
    "    # Loop over each model and its kernel\n",
    "    for model_name, optim_method in models_kernels:\n",
    "        loss_history = []\n",
    "        times = []\n",
    "        uncertainties = []  \n",
    "        memory_usages = []\n",
    "        \n",
    "        for idx, size in enumerate(dataset_sizes):\n",
    "            rmse, avg_uncertantiy, time_taken, pred_noise, in_interval = train_and_evaluate_gp(size, model_name, optim_method)\n",
    "            loss_history.append(rmse)\n",
    "            memory = estimate_memory_usage(n_samples=size, n_features=1, model_type=model_name, n_inducing=0.1*size)\n",
    "            uncertainties.append(avg_uncertantiy)\n",
    "            times.append(time_taken)\n",
    "            memory_usages.append(memory)\n",
    "            name = model_name + \" \" + optim_method\n",
    "            print(f\"Model: {name}, Dataset size: {size}, Time: {time_taken:.4f} seconds, \"\n",
    "                  f\"Memory: {memory:.2f} MB,\"\n",
    "                  f\"Uncertainty: {avg_uncertantiy:.4f}\")\n",
    "            \n",
    "            # Update dataframe\n",
    "            new_row = pd.DataFrame({\n",
    "                'Model': name,\n",
    "                'Dataset Size': size,\n",
    "                'RMSE': rmse,\n",
    "                'Time': time_taken,\n",
    "                'Memory Usage': memory,\n",
    "                'Avg Uncertainty': avg_uncertantiy,\n",
    "                'Predictive Noise': pred_noise,\n",
    "                'In Interval': in_interval,\n",
    "            }, index=[0])    \n",
    "\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "        \n",
    "        # Store results for this model\n",
    "        results['times'][name] = times\n",
    "        results['uncertainties'][name] = uncertainties\n",
    "        results['memory_usages'][name] = memory_usages\n",
    "        results['loss_history'][name] = loss_history\n",
    "    # Plotting the results\n",
    "\n",
    "    print(df.head())\n",
    "    df.to_csv('results.csv')\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    # Loss plot\n",
    "\n",
    "    plt.subplot(4, 1, 1)\n",
    "    for name in results['loss_history']:\n",
    "        plt.plot(dataset_sizes, results['loss_history'][name], 'o-', label=f'{name} Loss')\n",
    "    plt.title('Performance Metrics vs Dataset Size')\n",
    "    plt.ylabel('Loss (RMSE)')\n",
    "    plt.xscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Time Plot\n",
    "    plt.subplot(4, 1, 2)\n",
    "    for name in results['times']:\n",
    "        plt.plot(dataset_sizes, results['times'][name], 'o-', label=f'{name} Time')\n",
    "    plt.title('Performance Metrics vs Dataset Size')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.xscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Memory Usage Plot\n",
    "    plt.subplot(4, 1, 3)\n",
    "    for name in results['memory_usages']:\n",
    "        plt.plot(dataset_sizes, results['memory_usages'][name], 'o-', label=f'{name} Memory Usage')\n",
    "    plt.ylabel('Memory Usage (MB)')\n",
    "    plt.xscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Uncertainty Plot\n",
    "    plt.subplot(4, 1, 4)\n",
    "    for name in results['uncertainties']:\n",
    "        plt.plot(dataset_sizes, results['uncertainties'][name], 'o-', label=f'{name} Avg. Uncertainty')\n",
    "    plt.ylabel('Uncertainty')\n",
    "    plt.xscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SGPR L-BFGS-B, Dataset size: 50, Time: 1.0099 seconds, Memory: 0.00 MB,Uncertainty: 0.0756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gc/2hyygwnn1xq6gy01txr3nbj00000gn/T/ipykernel_14431/2329775923.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SGPR L-BFGS-B, Dataset size: 100, Time: 2.1261 seconds, Memory: 0.00 MB,Uncertainty: 0.1512\n",
      "Model: SGPR L-BFGS-B, Dataset size: 150, Time: 4.4085 seconds, Memory: 0.00 MB,Uncertainty: 0.0014\n",
      "Model: SGPR L-BFGS-B, Dataset size: 200, Time: 7.3235 seconds, Memory: 0.01 MB,Uncertainty: 0.0013\n",
      "Model: SGPR L-BFGS-B, Dataset size: 250, Time: 11.1711 seconds, Memory: 0.01 MB,Uncertainty: 0.0013\n",
      "Model: SGPR L-BFGS-B, Dataset size: 300, Time: 15.8849 seconds, Memory: 0.01 MB,Uncertainty: 0.0013\n",
      "Model: SGPR CG, Dataset size: 50, Time: 0.4833 seconds, Memory: 0.00 MB,Uncertainty: 0.0501\n",
      "Model: SGPR CG, Dataset size: 100, Time: 1.6724 seconds, Memory: 0.00 MB,Uncertainty: 0.1643\n",
      "Model: SGPR CG, Dataset size: 150, Time: 4.1380 seconds, Memory: 0.00 MB,Uncertainty: 2.1244\n",
      "Model: SGPR CG, Dataset size: 200, Time: 6.7473 seconds, Memory: 0.01 MB,Uncertainty: 0.0014\n",
      "Model: SGPR CG, Dataset size: 250, Time: 10.6091 seconds, Memory: 0.01 MB,Uncertainty: 0.0013\n",
      "Model: SGPR CG, Dataset size: 300, Time: 15.3476 seconds, Memory: 0.01 MB,Uncertainty: 0.0196\n",
      "Time budget exceeded ADAM.\n",
      "Parameter History:\n",
      "Model: SGPR ADAM, Dataset size: 50, Time: 10.4932 seconds, Memory: 0.00 MB,Uncertainty: 1.1923\n",
      "Time budget exceeded ADAM.\n",
      "Parameter History:\n",
      "Model: SGPR ADAM, Dataset size: 100, Time: 11.8889 seconds, Memory: 0.00 MB,Uncertainty: 1.1209\n",
      "Time budget exceeded ADAM.\n",
      "Parameter History:\n",
      "Model: SGPR ADAM, Dataset size: 150, Time: 13.7695 seconds, Memory: 0.00 MB,Uncertainty: 0.5512\n",
      "Time budget exceeded ADAM.\n",
      "Parameter History:\n",
      "Model: SGPR ADAM, Dataset size: 200, Time: 16.5211 seconds, Memory: 0.01 MB,Uncertainty: 0.6949\n",
      "Time budget exceeded ADAM.\n",
      "Parameter History:\n",
      "Model: SGPR ADAM, Dataset size: 250, Time: 20.3156 seconds, Memory: 0.01 MB,Uncertainty: 0.3458\n",
      "Time budget exceeded ADAM.\n",
      "Parameter History:\n"
     ]
    }
   ],
   "source": [
    "models_kernels = [\n",
    "    #(\"GPR\", \"L-BFGS-B\"),\n",
    "    (\"SGPR\", \"L-BFGS-B\"),\n",
    "    #(\"GPR\", \"CG\"),\n",
    "    (\"SGPR\", \"CG\"),\n",
    "    #(\"GPR\", \"ADAM\"),\n",
    "    (\"SGPR\", \"ADAM\"),\n",
    "]\n",
    "# Run the comparison for these models\n",
    "run_comparison(max_samples=300, models_kernels=models_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
